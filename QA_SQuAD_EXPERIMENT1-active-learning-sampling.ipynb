{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YzK857fXfVbm"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4958f95e1c4bfdb74001c17cfdd372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tB0XF6GifVbq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.35.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, default_data_collator\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "squad_v2 = True\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 32\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "n_best_size = 20\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets1={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [],
   "source": [
    "datasets1['train'] = load_dataset(\"squad_v2\", split='train[:90%]' if squad_v2 else \"squad\")\n",
    "datasets1['validation'] = load_dataset(\"squad_v2\", split='train[90%:100%]' if squad_v2 else \"squad\")\n",
    "datasets1['test'] = load_dataset(\"squad_v2\", split='train' if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 117287\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 13032\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 130319\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "datasets= DatasetDict(datasets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5730b5b4069b531400832297',\n",
       " 'title': 'Sumer',\n",
       " 'context': 'Sumerian religion seems to have been founded upon two separate cosmogenic myths. The first saw creation as the result of a series of hieros gami or sacred marriages, involving the reconciliation of opposites, postulated as a coming together of male and female divine beings; the gods. This continued to influence the whole Mesopotamian mythos. Thus in the Enuma Elish the creation was seen as the union of fresh and salt water; as male Abzu, and female Tiamat. The product of that union, Lahm and Lahmu, \"the muddy ones\", were titles given to the gate keepers of the E-Abzu temple of Enki, in Eridu, the first Sumerian city. Describing the way that muddy islands emerge from the confluence of fresh and salty water at the mouth of the Euphrates, where the river deposited its load of silt, a second hieros gamos supposedly created Anshar and Kishar, the \"sky-pivot\" or axle, and the \"earth pivot\", parents in turn of Anu (the sky) and Ki (the earth). Another important Sumerian hieros gamos was that between Ki, here known as Ninhursag or \"Lady Sacred Mountain\", and Enki of Eridu, the god of fresh water which brought forth greenery and pasture.',\n",
       " 'question': 'In the Enuma Elish, what is creation seen as the union of?',\n",
       " 'answers': {'text': ['fresh and salt water'], 'answer_start': [406]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "# Show random elements in the dataset\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57277c575951b619008f8b14</td>\n",
       "      <td>Tuberculosis</td>\n",
       "      <td>It is the most widely used vaccine worldwide, with more than 90% of all children being vaccinated. The immunity it induces decreases after about ten years. As tuberculosis is uncommon in most of Canada, the United Kingdom, and the United States, BCG is administered only to those people at high risk. Part of the reasoning against the use of the vaccine is that it makes the tuberculin skin test falsely positive, reducing the test's use in screening. A number of new vaccines are currently in development.</td>\n",
       "      <td>What percentage of kids in the whole world get the BCG vaccine?</td>\n",
       "      <td>{'text': ['90%'], 'answer_start': [61]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a847d907cf838001a46a864</td>\n",
       "      <td>Endangered_Species_Act</td>\n",
       "      <td>The ESA requires that critical habitat be designated at the time of or within one year of a species being placed on the endangered list. In practice, most designations occur several years after listing. Between 1978 and 1986 the FWS regularly designated critical habitat. In 1986 the Reagan Administration issued a regulation limiting the protective status of critical habitat. As a result, few critical habitats were designated between 1986 and the late 1990s. In the late 1990s and early 2000s, a series of court orders invalidated the Reagan regulations and forced the FWS and NMFS to designate several hundred critical habitats, especially in Hawaii, California and other western states. Midwest and Eastern states received less critical habitat, primarily on rivers and coastlines. As of December, 2006, the Reagan regulation has not yet been replaced though its use has been suspended. Nonetheless, the agencies have generally changed course and since about 2005 have tried to designate critical habitat at or near the time of listing.</td>\n",
       "      <td>What did the Reagan regulations invalidate?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>571a5a3d10f8ca1400304feb</td>\n",
       "      <td>Ashkenazi_Jews</td>\n",
       "      <td>It is estimated that in the 11th century Ashkenazi Jews composed only three percent of the world's Jewish population, while at their peak in 1931 they accounted for 92 percent of the world's Jews. Immediately prior to the Holocaust, the number of Jews in the world stood at approximately 16.7 million. Statistical figures vary for the contemporary demography of Ashkenazi Jews, oscillating between 10 million and 11.2 million. Sergio DellaPergola in a rough calculation of Sephardic and Mizrahi Jews, implies that Ashkenazi make up less than 74% of Jews worldwide. Other estimates place Ashkenazi Jews as making up about 75% of Jews worldwide.</td>\n",
       "      <td>By 1931, what percentage of the world’s Jews were Ashkenazi Jews?</td>\n",
       "      <td>{'text': ['92 percent'], 'answer_start': [165]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57281a552ca10214002d9dd7</td>\n",
       "      <td>Central_African_Republic</td>\n",
       "      <td>Michel Djotodia took over as president and in May 2013 Central African Republic's Prime Minister Nicolas Tiangaye requested a UN peacekeeping force from the UN Security Council and on 31 May former President Bozizé was indicted for crimes against humanity and incitement of genocide. The security situation did not improve during June–August 2013 and there were reports of over 200,000 internally displaced persons (IDPs) as well as human rights abuses and renewed fighting between Séléka and Bozizé supporters.</td>\n",
       "      <td>What was Bozize indicted for?</td>\n",
       "      <td>{'text': ['crimes against humanity'], 'answer_start': [232]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56f95a099b226e1400dd1389</td>\n",
       "      <td>Marshall_Islands</td>\n",
       "      <td>Under German control, and even before then, Japanese traders and fishermen from time to time visited the Marshall Islands, although contact with the islanders was irregular. After the Meiji Restoration (1868), the Japanese government adopted a policy of turning the Japanese Empire into a great economic and military power in East Asia.</td>\n",
       "      <td>In what region did the Japanese attempt to build up their power after 1868?</td>\n",
       "      <td>{'text': ['East Asia'], 'answer_start': [326]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5726cf69f1498d1400e8ebed</td>\n",
       "      <td>The_Sun_(United_Kingdom)</td>\n",
       "      <td>In August 2013, The Irish Sun ended the practice of featuring topless models on Page 3. The main newspaper was reported to have followed in 2015 with the edition of 16 January supposedly the last to carry such photographs after a report in The Times made such an assertion. After substantial coverage in the media about an alleged change in editorial policy, Page 3 returned to its usual format on 22 January 2015. A few hours before the issue was published, the Head of PR at the newspaper said the reputed end of Page 3 had been \"speculation\" only.</td>\n",
       "      <td>What was stated to be the last edition of the regular Sun to publish topless models?</td>\n",
       "      <td>{'text': ['the edition of 16 January'], 'answer_start': [150]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5ad41e54604f3c001a400637</td>\n",
       "      <td>Houston</td>\n",
       "      <td>The Houston area has over 150 active faults (estimated to be 300 active faults) with an aggregate length of up to 310 miles (500 km), including the Long Point–Eureka Heights fault system which runs through the center of the city. There have been no significant historically recorded earthquakes in Houston, but researchers do not discount the possibility of such quakes having occurred in the deeper past, nor occurring in the future. Land in some areas southeast of Houston is sinking because water has been pumped out of the ground for many years. It may be associated with slip along the faults; however, the slippage is slow and not considered an earthquake, where stationary faults must slip suddenly enough to create seismic waves. These faults also tend to move at a smooth rate in what is termed \"fault creep\", which further reduces the risk of an earthquake.</td>\n",
       "      <td>How many estimated unactive faults does Houston have?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56df76c85ca0a614008f9ac5</td>\n",
       "      <td>Alexander_Graham_Bell</td>\n",
       "      <td>Bell's father, grandfather, and brother had all been associated with work on elocution and speech, and both his mother and wife were deaf, profoundly influencing Bell's life's work. His research on hearing and speech further led him to experiment with hearing devices which eventually culminated in Bell being awarded the first U.S. patent for the telephone in 1876.[N 4] Bell considered his most famous invention an intrusion on his real work as a scientist and refused to have a telephone in his study.[N 5]</td>\n",
       "      <td>What year did Bell get the patent for his telephone?</td>\n",
       "      <td>{'text': ['1876'], 'answer_start': [361]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5726a578f1498d1400e8e5f6</td>\n",
       "      <td>Pope_Paul_VI</td>\n",
       "      <td>His organisational skills led him to a career in the Roman Curia, the papal civil service. In 1931, Pacelli appointed him to teach history at the Papal Academy for Diplomats In 1937, after his mentor Giuseppe Pizzardo was named a cardinal and was succeeded by Domenico Tardini, Montini was named Substitute for Ordinary Affairs under Cardinal Pacelli, the Secretary of State under Pope Pius XI. From Pius XI, whom he viewed with awe, he adopted the view, that learning is a life long process, and that history was the magister vitae teacher of life His immediate supervisor in the Vatican was Domenico Tardini, with whom he got along well. The election of Pacelli to the papacy in 1939, anticipated by everybody and openly promoted by Pope Pius XI in his last years, was a good omen for Montini, whose position was confirmed in the position under the new Cardinal Secretary of State Luigi Maglione. He met the pope every morning until 1954 and thus developed a rather close relationship:</td>\n",
       "      <td>With whom did Montini have a relationship with that eventually groomed him for the papacy?</td>\n",
       "      <td>{'text': ['Pacelli'], 'answer_start': [656]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>572677e5dd62a815002e8607</td>\n",
       "      <td>Florida</td>\n",
       "      <td>At the end of the third quarter in 2008, Florida had the highest mortgage delinquency rate in the country, with 7.8% of mortgages delinquent at least 60 days. A 2009 list of national housing markets that were hard hit in the real estate crash included a disproportionate number in Florida. The early 21st-century building boom left Florida with 300,000 vacant homes in 2009, according to state figures. In 2009, the US Census Bureau estimated that Floridians spent an average 49.1% of personal income on housing-related costs, the third highest percentage in the country.</td>\n",
       "      <td>How many vacant homes in Florida in 2009</td>\n",
       "      <td>{'text': ['300,000 vacant homes in 2009'], 'answer_start': [345]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "stRrZQwzfVb5"
   },
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 19538, 4430, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Minhah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ft1wuf0afVcF"
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hCk0uR8pfVcG",
    "outputId": "262f68f0-c156-4fef-e9fb-c4f34e4fd756"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SvXWJtG2fVcH",
    "outputId": "89f6067e-adb1-4eef-8f56-024da04fad4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9IJtWAfcfVcI"
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gsL3HIlEfVcJ",
    "outputId": "67d08cea-8804-45a1-e20d-0e32806ecaf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 192]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tYMkxcazfVcJ",
    "outputId": "4b5935ac-ddef-4c3a-e222-06e015917753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n",
      "[CLS] beyonce got married in 2008 to whom? [SEP] single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift's award during her own acceptance speech. in march 2009, beyonce embarked on the i am... world tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $ 119. 5 million. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r3GF8tB1fVcK",
    "outputId": "7952e0ab-4420-4354-e12a-10d112985330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 153), (153, 154), (154, 155), (156, 161), (162, 168), (168, 169), (170, 172), (173, 182), (182, 183), (183, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 221), (221, 222), (222, 223), (224, 229), (230, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 330), (330, 331), (331, 332), (333, 338), (339, 342), (343, 348), (349, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396), (397, 403)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YpO8OoLEfVcL",
    "outputId": "8a163632-7243-4166-eb91-daeac761659e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beyonce Beyonce\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_duFgZQVfVcL",
    "outputId": "cb31f357-d755-4762-8bd6-6a08060f12ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Sr4VJegAfVcM",
    "outputId": "99d79ca7-6cdf-4286-ab04-c952c09ff051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 19\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MsNZeNEWfVcN",
    "outputId": "386a8798-2ae7-42b1-bd84-d4d6f8f6bd7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jay z\n",
      "Jay Z\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4oCcR7K1fVcN"
   },
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"test-squad-trained\")\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bKaODdh9fVcO"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the training data\n",
    "def prepare_train_features(examples):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if tokenizer.padding_side == \"right\" else \"context\"],\n",
    "        examples[\"context\" if tokenizer.padding_side == \"right\" else \"question\"],\n",
    "        truncation=\"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if tokenizer.padding_side == \"right\" else 0):\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if tokenizer.padding_side == \"right\" else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-b70jh26IrJS"
   },
   "outputs": [],
   "source": [
    "features = prepare_train_features(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "# Map training features to GPU\n",
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iCrdHFmhfVcT"
   },
   "outputs": [],
   "source": [
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MIc4PLzifVcV"
   },
   "outputs": [],
   "source": [
    "# trainer.save_model(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp-DPmjwfVcV"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "9lZY2OhqfVcV",
    "outputId": "382f5bb8-5e8e-4d0c-b908-c0f0c022ce33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jZ8gd5oHfVcW",
    "outputId": "3a2a2510-4eef-4d74-9240-551ed70a71a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OoYqxrPIfVcW",
    "outputId": "39a3e87b-b195-4df4-b690-4adf217e69da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([102, 127, 257,   0,  14,  34,  55,  66, 102,  31,  37,  52,  69,  10,\n",
       "          45,  65,  77,  94,   0,   0,   0,   0,  31,  55, 117, 142,   0,  64,\n",
       "          45,  97, 135,  30,  42,   0,   0,  26,  28,  52,  63,   0,   0,  76,\n",
       "           0,  20,  24,  78, 106, 166,   0,  18,   0, 139,  50,  52,  55,  92,\n",
       "         152,   0,   0,   0,  26,  75,  75,  88], device='cuda:0'),\n",
       " tensor([105,   0, 260,   0,  15,  35,  56,  71, 106,  34,  51,  53,  70,  13,\n",
       "          45,  66,  80,  96,   0,   0,   0,   0,  34,  61, 118, 142,  25,  65,\n",
       "          46,  99, 138,  37,  43,   0,   0,  26,  29,  59,  64,   0,   0,   0,\n",
       "           0,  24,  43,  79, 106, 166,   0,  24,   0, 141,  53,  54,  55,  94,\n",
       "         158,   0,   0,   0,  29,  76,  76,  93], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nxJw6QHIfVcX"
   },
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DyJojyQ8fVcX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ptCUNWL8fVcX"
   },
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "32ba04d6240149f49eb48c8d8b7f9aae"
     ]
    },
    "id": "fpuPA5VlfVcY",
    "outputId": "fb779dee-8a69-47d0-ed00-61865eaccdee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7709406271941e3bf5e6d5e0d557a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map validation features to GPU\n",
    "validation_features = datasets[\"test\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"test\"].column_names\n",
    ")\n",
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jHU5I9wwfVcZ",
    "outputId": "ce3c3c7e-254b-4ad3-9a6f-0ff98754c55a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rabeea/anaconda3/envs/tor/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 1. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictions on GPU\n",
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "W8Oz3BfzfVcZ"
   },
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ddHVfq4yfVca"
   },
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jLZy0VYJfVca",
    "outputId": "c5544a69-b00d-48a0-91f5-890bd5052784"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 15.840794, 'text': 'the group became one'},\n",
       " {'score': 13.062016, 'text': 'Knowles, the group became one'},\n",
       " {'score': 10.543509, 'text': 'Mathew Knowles, the group became one'},\n",
       " {'score': 9.170052, 'text': 'the group became one of'},\n",
       " {'score': 8.622179,\n",
       "  'text': \"the group became one of the world's best-selling girl groups of\"},\n",
       " {'score': 8.147081, 'text': 'one'},\n",
       " {'score': 7.791733,\n",
       "  'text': \"the group became one of the world's best-selling girl groups\"},\n",
       " {'score': 7.053434, 'text': \"the group became one of the world's\"},\n",
       " {'score': 6.473832,\n",
       "  'text': \"the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's\"},\n",
       " {'score': 6.41477,\n",
       "  'text': \"the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release\"},\n",
       " {'score': 6.3912745, 'text': 'Knowles, the group became one of'},\n",
       " {'score': 6.371533, 'text': 'the'},\n",
       " {'score': 5.8434024,\n",
       "  'text': \"Knowles, the group became one of the world's best-selling girl groups of\"},\n",
       " {'score': 5.65989, 'text': 'became one'},\n",
       " {'score': 5.634647,\n",
       "  'text': 'by her father, Mathew Knowles, the group became one'},\n",
       " {'score': 5.5679455,\n",
       "  'text': 'Managed by her father, Mathew Knowles, the group became one'},\n",
       " {'score': 5.0129557,\n",
       "  'text': \"Knowles, the group became one of the world's best-selling girl groups\"},\n",
       " {'score': 4.274657, 'text': \"Knowles, the group became one of the world's\"},\n",
       " {'score': 3.872767, 'text': 'Mathew Knowles, the group became one of'},\n",
       " {'score': 3.695055,\n",
       "  'text': \"Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's\"}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"test\"][0][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "M19mLC9KfVcb",
    "outputId": "214d7ff7-d33c-4603-d148-11bdffa34e88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['in the late 1990s'], 'answer_start': [269]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"test\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_zKZ41AQfVcc"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"test\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "qVodHvy5fVcc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Postprocess QA predictions\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Map validation features to GPU\n",
    "validation_features = datasets[\"test\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"test\"].column_names\n",
    ")\n",
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "\n",
    "# Predictions on GPU\n",
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "347ebed36d3541388e4e821372e91aa4"
     ]
    },
    "id": "6AXKZ14ZfVcd",
    "outputId": "482729b6-266c-437e-f982-790f57a09dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 130319 example predictions split into 131754 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0573b747c14c22918f20906f9837dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Postprocess predictions\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"test\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "doWt2cb4fVce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518955/2905994612.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "SolYNyTFfVce",
    "outputId": "711f66b3-1c50-449f-8580-8a4153944d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 86.66119291891435,\n",
       " 'f1': 91.49233744750889,\n",
       " 'total': 130319,\n",
       " 'HasAns_exact': 83.94397668766773,\n",
       " 'HasAns_f1': 91.19556240796064,\n",
       " 'HasAns_total': 86821,\n",
       " 'NoAns_exact': 92.0846935491287,\n",
       " 'NoAns_f1': 92.0846935491287,\n",
       " 'NoAns_total': 43498,\n",
       " 'best_exact': 86.72641748325263,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 91.55756201184958,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "\n",
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"test\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Specify the file path where you want to save the data\n",
    "file_path = \"references.json\"\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(references, json_file)\n",
    "# Specify the file path where you want to save the data\n",
    "file_path = \"formatted_predictions.json\"\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(formatted_predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate F1 score\n",
    "def compute_f1_score(prediction, reference):\n",
    "    predicted_tokens = tokenizer.tokenize(prediction)\n",
    "    reference_tokens = tokenizer.tokenize(reference)\n",
    "    common_tokens = set(predicted_tokens) & set(reference_tokens)\n",
    "    all_tokens = set(predicted_tokens) | set(reference_tokens)\n",
    "    precision = len(common_tokens) / len(all_tokens) if len(all_tokens) > 0 else 0.0\n",
    "    recall = len(common_tokens) / len(reference_tokens) if len(reference_tokens) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7675912635454ae091d5370671e61e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Calculate F1 score for each example\n",
    "f1_scores = []\n",
    "for pred in tqdm(formatted_predictions,desc='processing'):\n",
    "    reference = next(ref for ref in references if ref[\"id\"] == pred[\"id\"])\n",
    "    reference_text = reference[\"answers\"][\"text\"] if reference[\"answers\"][\"text\"] else ['']\n",
    "#     print((reference_text, pred[\"prediction_text\"]))\n",
    "    f1 = compute_f1_score(reference_text[0], pred[\"prediction_text\"])\n",
    "    f1_scores.append({\"id\": pred[\"id\"], \"f1_score\": f1})\n",
    "\n",
    "# Sort examples based on descending F1 scores\n",
    "sorted_examples = sorted(f1_scores, key=lambda x: x[\"f1_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dc000dd7a54183901baaf55a146691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee6c264c2e6463f8d48a31810f450e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07bf76b76a54ce6ad3ee209b92be8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the scoring functions\n",
    "epsilon = 0.01\n",
    "\n",
    "def hard_score(f1):\n",
    "    return 1 - f1 + epsilon\n",
    "\n",
    "def moderate_score(f1):\n",
    "    return 2 - f1\n",
    "\n",
    "def soft_score(f1):\n",
    "    return 3 - f1\n",
    "\n",
    "# Calculate probability weights for each example\n",
    "total_examples = len(sorted_examples)\n",
    "hard_weights = [hard_score(example[\"f1_score\"]) for example in tqdm(sorted_examples,desc='processing')]\n",
    "moderate_weights = [moderate_score(example[\"f1_score\"]) for example in tqdm(sorted_examples,desc='processing')]\n",
    "soft_weights = [soft_score(example[\"f1_score\"]) for example in tqdm(sorted_examples,'processing')]\n",
    "\n",
    "# Normalize weights\n",
    "hard_weights /= np.sum(hard_weights)\n",
    "moderate_weights /= np.sum(moderate_weights)\n",
    "soft_weights /= np.sum(soft_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df5e11af7cd4a1d8f014c9b0925e4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb708d52f4bc42eaa3bcf70cce31a12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8cdb0e2cea4095a94340d9b997465b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample 5000 examples based on the computed weights\n",
    "num_samples = 5000\n",
    "sampled_indices = np.random.choice(total_examples, size=num_samples, p=hard_weights, replace=True)\n",
    "sampled_examples = [sorted_examples[i] for i in tqdm(sampled_indices,desc='processing')]\n",
    "\n",
    "# Extract the sampled IDs\n",
    "sampled_ids = [example[\"id\"] for example in tqdm(sampled_examples,desc='processing')]\n",
    "\n",
    "# Retrieve the corresponding examples from the dataset\n",
    "sampled_data_hard = [example for example in tqdm(datasets[\"test\"],desc='processing') if example[\"id\"] in sampled_ids]\n",
    "\n",
    "file_path = \"sampled_data_hard.json\"\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(sampled_data_hard, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14fcd1fa0bd4bc18bf8b7e85196ad98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdhdict = Dataset.from_pandas(pd.DataFrame(sampled_data_hard))\n",
    "save_path = \"dataset_hard\"\n",
    "\n",
    "# Save the dataset to the specified path\n",
    "sdhdict.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f7bb9189524198a8ee0414f2b7187d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a59911b4a5c42a388adaa4139d7576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a7cad6b6804df2b3dc265b1a21e4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample 5000 examples based on the computed weights\n",
    "num_samples = 5000\n",
    "sampled_indices = np.random.choice(total_examples, size=num_samples, p=moderate_weights, replace=True)\n",
    "sampled_examples = [sorted_examples[i] for i in tqdm(sampled_indices,desc='processing')]\n",
    "\n",
    "# Extract the sampled IDs\n",
    "sampled_ids = [example[\"id\"] for example in tqdm(sampled_examples,desc='processing')]\n",
    "\n",
    "# Retrieve the corresponding examples from the dataset\n",
    "sampled_data_moderate = [example for example in tqdm(datasets[\"test\"],desc='processing') if example[\"id\"] in sampled_ids]\n",
    "\n",
    "file_path = \"sampled_data_moderate.json\"\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(sampled_data_moderate, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf2297994d14daba616f8e12d418116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4907 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdmdict = Dataset.from_pandas(pd.DataFrame(sampled_data_moderate))\n",
    "save_path = \"dataset_moderate\"\n",
    "\n",
    "# Save the dataset to the specified path\n",
    "sdmdict.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f699c7dce3794c87ad2181d2c28ac933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f87f79d8a28437ead8f6e61ab181d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fcbbb597a24dfc9d3e42a751dc827a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/130319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample 5000 examples based on the computed weights\n",
    "num_samples = 5000\n",
    "sampled_indices = np.random.choice(total_examples, size=num_samples, p=soft_weights, replace=True)\n",
    "sampled_examples = [sorted_examples[i] for i in tqdm(sampled_indices,desc='processing')]\n",
    "\n",
    "# Extract the sampled IDs\n",
    "sampled_ids = [example[\"id\"] for example in tqdm(sampled_examples,desc='processing')]\n",
    "\n",
    "# Retrieve the corresponding examples from the dataset\n",
    "sampled_data_soft = [example for example in tqdm(datasets[\"test\"],desc='processing') if example[\"id\"] in sampled_ids]\n",
    "\n",
    "file_path = \"sampled_data_soft.json\"\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(sampled_data_soft, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af759027861043db9d54bb18b2516779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdsdict = Dataset.from_pandas(pd.DataFrame(sampled_data_soft))\n",
    "save_path = \"dataset_soft\"\n",
    "\n",
    "# Save the dataset to the specified path\n",
    "sdsdict.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tor] *",
   "language": "python",
   "name": "conda-env-tor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
